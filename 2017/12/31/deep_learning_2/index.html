<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="励志成为一位大佬">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    深度学习初探一——奇偶数学习 |
    
    春少 Blog</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
</head>

<body>
<main class="content">
  <section class="outer">
  <article id="post-deep_learning_2" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

    <div class="article-inner">
        
            <header class="article-header">
                
  
    <h1 class="article-title" itemprop="name">
      深度学习初探一——奇偶数学习
    </h1>
  
  




            </header>
            

                
                    <div class="article-meta">
                        <a href="/2017/12/31/deep_learning_2/" class="article-date">
  <time datetime="2017-12-31T20:11:48.000Z" itemprop="datePublished">2017-12-31</time>
</a>
                            
  <div class="article-category">
    <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
  </div>

                    </div>
                    

                        
                            




                                

                                    <div class="article-entry" itemprop="articleBody">
                                        


                                            

                                                
                                                                    <p>奇偶数判断，这个是不是太简单了，不就一个数去对2取取余数，如果余数为0，那么这个数为偶数，否则，这个数为奇数，用Python写的话代码就像这样：</p>
<pre><code>def fun(number):
    if number % 2 == 0:
        print &quot;这是偶数&quot;
    else:
        print &quot;这是奇数&quot;
</code></pre><p>但是，这个判断法则是我们知道的，我们人为的在编程中将判断规则告诉计算机，通过计算来判断一个数是奇数还是偶数。如果我们要只告诉计算机一些数中那些是奇数，那些是偶数，然后让它自己判断新的数据属于偶数还是奇数的话，你要怎么编程呢？这不，最近闲来无视，看了看深度学习的书，突然想试一下，用深度神经网络让计算机去学习奇偶数的特点，然后去判断新的数据是奇数还是偶数。</p>
<a id="more"></a>
<p>现在我们来讲重点！！！如何用深度神经网络来让计算机学习判断一个数的奇偶性。</p>
<blockquote>
<p>First Step</p>
</blockquote>
<p>我们要训练一个神经网络，总要有数据吧？现在，我们就来创造一些训练数据集，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> pp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(number=<span class="number">10001</span>)</span>:</span></span><br><span class="line">    temp = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, number * <span class="number">2</span>)]</span><br><span class="line">    data, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> temp:</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            data.append([i, i % <span class="number">10</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data.append([i, i % <span class="number">10</span>, <span class="number">0</span>])</span><br><span class="line">    seclar = pp.MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    data = seclar.fit_transform(data)</span><br><span class="line">    np.random.shuffle(data)</span><br><span class="line">    <span class="keyword">print</span> data</span><br><span class="line">    labels = data[:, <span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> data, labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    load_data()</span><br></pre></td></tr></table></figure></p>
<p>我们在这个py文件中导入了<a href="https://wizardforcel.gitbooks.io/ts-numpy-tut/content/" target="_blank" rel="noopener">numpy</a>包和<a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/" target="_blank" rel="noopener">sklearn</a>包，这两个包是在深度学习中常用的到，numpy用于数据的处理方面，sklearn是一个机器学习模块。我们先通过python自带的列表生成式生成数据范围在[1，number]的list数据类型，然后通过np.array()将我们list对象转为numpy的核心对象ndarry，同时，我们也为每一个数生成标签，如果是偶数，那么这个数的labels是1，否则为0，这样我们就完成了数据集的构建，接下来我们看到这几段代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seclar = pp.MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">data = seclar.fit_transform(data)</span><br><span class="line">np.random.shuffle(data)</span><br></pre></td></tr></table></figure></p>
<p>这几段代码什么意思呢？我们简要分析下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seclar = pp.MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p>这行代码是设置了归一化，我们对numpy.ndarry的同一列数据进行归一化操作，所谓归一化就是将所有数据缩小到[0，1]的范围内，那么归一化的数学公式呢？看下图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_num = (old_num - min_value) / (max_value - min_value)</span><br></pre></td></tr></table></figure></p>
<p>这就是归一化前和归一化后的区别：</p>
<p><img src="https://liaochuntao.files.wordpress.com/2017/12/screenshot-from-2018-01-01-01-24-51.png" alt="Screenshot from 2018-01-01 01-24-51"></p>
<p>为什么要进行归一化呢？这里引用这篇<a href="http://www.cnblogs.com/subsir/articles/4761140.html" target="_blank" rel="noopener">博客（数据标准化和归一化）</a>的原文：</p>
<blockquote>
<p>归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。归一化是为了加快训练网络的收敛性，可以不进行归一化处理</p>
<p>归一化的具体作用是归纳统一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1–+1之间是统计的坐标分布。归一化有同一、统一和合一的意思。无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练(概率计算)和预测的，归一化是同一在0-1之间的统计概率分布;SVM是以降维后线性划分距离来分类和仿真的，因此时空降维归一化是统一在-1–+1之间的统计坐标分布。</p>
</blockquote>
<p>现在，我们还需要将每一个数的标签单独提取出来，最为结果用于训练模型与模型预测结果比对：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels = data[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>
<p>这样，我们的前期工作就准备完毕了</p>
<blockquote>
<p>Second Step</p>
</blockquote>
<p>这一步，我们就开始来构建我们的神经网络模型了，读者可以先去<a href="http://keras-cn.readthedocs.io/en/latest/" target="_blank" rel="noopener">keras中文</a>了解下tensorflow高级封装的深度学习框架，这个框架适合快速上手深度学习，但是还是要回归tensorflow。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> load_data</span><br></pre></td></tr></table></figure></p>
<p>我们先导入相关的包，这是等等构建模型需要用到的，像全连接层：Dense、按一定概率随机断开神经元：Dropout、序贯模型：Sequential、绘图类：pyplot以及上一步我们的创建数据的包：load_data</p>
<p>先放上构建模型的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">built_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment"># first layers</span></span><br><span class="line">    model.add(Dense(units=<span class="number">256</span>,</span><br><span class="line">                    input_dim=<span class="number">2</span>,</span><br><span class="line">                           activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># second layers</span></span><br><span class="line">    model.add(Dense(units=<span class="number">128</span>,</span><br><span class="line">                    activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># third layers</span></span><br><span class="line">    model.add(Dense(units=<span class="number">1</span>,</span><br><span class="line">                    activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">    model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                  optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    model.summary()</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>我们这个模型有三层神经网络——三个全连接层（Dense），每个全连接层加了Dropout层，Dense层的作用我在上一篇文章<a href="https://liaochuntao.wordpress.com/2017/12/27/%e8%ae%b0%e4%b8%80%e6%ac%a1%e7%ab%9e%e8%b5%9b%e9%a2%98%e7%9b%ae%e6%9c%89%e6%84%9f-%e5%9b%be%e5%83%8f%e8%af%86%e5%88%ab/" target="_blank" rel="noopener">《记一次竞赛题目有感——图像识别》</a> 中解释了下Dense层的作用，但是对于Dropout层的作用我只是说了下防止模型过拟合情况，那么怎么防止这个情况呢？看来还要从全连接层中说起，首先我先放上这个模型的图示以及之前没说过的全连接层的函数表达形式：</p>
<blockquote>
<p>学习奇偶数的模型图示</p>
</blockquote>
<p><img src="https://liaochuntao.files.wordpress.com/2017/12/screenshot-from-2017-12-31-19-21-00.png" alt="Screenshot from 2017-12-31 19-21-00"></p>
<blockquote>
<p>y = ∑i （wi * xi） + b  这个模型的第一、二层全连接层函数表达式</p>
</blockquote>
<p>由于图的问题，全连接层的神经元个数我并没有全部画出来来，因为全部画出来有点小多……这里就是一个简要的模型图，我们可以分清楚的看出我们的模型输入数据维度是二维的，表示我们对一个数字提取了两个特征，然后就是熟悉的全连接层的操作：每个神经元与下一层的所用神经元都有连接；接下来，我们看到在第一层与第二层的全连接层中，标注了一个记号——Dropout层作用的地方，没错，Dropout层作用的地方就是在每两层Dense层之间；为什么作用在这里？因为上一层的每个神经元与下一层的所有神经元都会连接，每个连接之间会有一个权值wi，连接数量越多，权值wi数量也会越多，而权值wi的取值其实就是这个模型的函数的待定系数取值，这个函数的含义其实就是去尽可能的描述一个数与奇偶性的关系，如果说权值wi越多，那么抓住的信息也就越多，就越能学习之间的关系，但是这不就是我们想要的吗？我们在把视线回到模型的开始之处——输入特征向量矩阵，整个模型的学习，是通过给的训练数据和对应答案来学习这二者之间的关系的，如果说我们的权值wi越多，其实学习到的关系是仅仅训练数据与答案之间的关系，相当于我们限定了自变量x的范围，我们在一个闭区间中让我们的模型去学习，那么最终，我们的模型学到的东西，也仅仅局限于这个闭区间中的内容，如果数据不在这个区间呢？那么模型就没办法处理了，这就是我们所说的模型过拟合问题，其原因就是全连接层之间神经元连接数太多导致权值wi数量也相应的增加。那么我们就想到能不能减少神经元之间的连接数量？这个工作的实现就是Dropout层来实现的，Dropout层的工作是一定几率的将某些神经元之间的连接断开（也就是让输入为0），这样我们就减少了权值wi的个数，也就一定程度上防止了我们的模型过拟合。</p>
<blockquote>
<p>最终完整代码</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> load_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">built_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Dense(units=<span class="number">256</span>,</span><br><span class="line">                    input_dim=<span class="number">2</span>,</span><br><span class="line">                    activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(units=<span class="number">128</span>,</span><br><span class="line">                    activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(units=<span class="number">1</span>,</span><br><span class="line">                    activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">    model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                  optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    model.summary()</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(batch_size=<span class="number">32</span>, verbose=<span class="number">2</span>, validation_split=<span class="number">0.2</span>, model=None)</span>:</span></span><br><span class="line">    data, labels = load_data.load_data()</span><br><span class="line">    train_x, train_y = data[: int(len(data) * <span class="number">0.7</span>), <span class="number">0</span>: <span class="number">2</span>], labels[: int(len(labels) * <span class="number">0.7</span>)]</span><br><span class="line">    test_x, test_y = data[int(len(data) * <span class="number">0.7</span>): len(data), <span class="number">0</span>: <span class="number">2</span>], labels[int(len(labels) * <span class="number">0.7</span>): len(labels)]</span><br><span class="line">    <span class="keyword">if</span> model <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        model = built_model()</span><br><span class="line">        history = model.fit(train_x, train_y,</span><br><span class="line">                            batch_size=batch_size,</span><br><span class="line">                            epochs=<span class="number">10</span>,</span><br><span class="line">                            verbose=verbose,</span><br><span class="line">                            validation_split=validation_split)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"刻画损失函数的变化趋势"</span></span><br><span class="line">        plt.plot(history.history[<span class="string">'loss'</span>], label=<span class="string">'train'</span>)</span><br><span class="line">        plt.plot(history.history[<span class="string">'val_loss'</span>], label=<span class="string">'valid'</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"模型构建成功，开始预测数据"</span></span><br><span class="line">        score = model.evaluate(test_x, test_y,</span><br><span class="line">                               batch_size=batch_size)</span><br><span class="line">        <span class="keyword">print</span> score</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"画图"</span></span><br><span class="line">        predicted = model.predict(test_x,</span><br><span class="line">                                  batch_size=batch_size,</span><br><span class="line">                                  verbose=<span class="number">0</span>)</span><br><span class="line">        rounded = [round(w) <span class="keyword">for</span> w <span class="keyword">in</span> predicted]</span><br><span class="line">        plt.scatter(test_y, list(range(len(test_y))), marker=<span class="string">'+'</span>)</span><br><span class="line">        plt.scatter(rounded, list(range(len(predicted))), marker=<span class="string">'*'</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    train_model()</span><br></pre></td></tr></table></figure>
<p>损失函数值的变化：</p>
<p><img src="https://liaochuntao.files.wordpress.com/2017/12/loss2.png" alt="loss"></p>
<blockquote>
<p>YouTube站点查看模型构建视频：</p>
</blockquote>
<p>[youtube <a href="https://www.youtube.com/watch?v=ZcS0lMsmKMY&amp;w=560&amp;h=315]" target="_blank" rel="noopener">https://www.youtube.com/watch?v=ZcS0lMsmKMY&amp;w=560&amp;h=315]</a></p>
<blockquote>
<p>哔哩哔哩站点查看模型构建视频：</p>
</blockquote>
<p><a href="https://www.bilibili.com/video/av17838797/" target="_blank" rel="noopener">https://www.bilibili.com/video/av17838797/</a></p>
<blockquote>
<p>源代码地址：</p>
</blockquote>
<p><a href="https://github.com/chuntaojun/Deeplearning/tree/master/Parity%20learning" target="_blank" rel="noopener">https://github.com/chuntaojun/Deeplearning/tree/master/Parity%20learning</a></p>

                                                                        
                                    </div>
                                    <footer class="article-footer">
                                        <a data-url="https://www.liaochuntao.cn/2017/12/31/deep_learning_2/" data-id="ckbai2qbp001g16ol1bpkosuu" class="article-share-link">
                                            Share
                                        </a>
                                        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/keras/">keras</a></li></ul>

                                    </footer>

    </div>

    
        
  <nav class="article-nav">
    
      <a href="/2018/01/09/java_web_6/" class="article-nav-link">
        <strong class="article-nav-caption">Newer</strong>
        <div class="article-nav-title">
          
            如何简单实现JDK的动态代理功能
          
        </div>
      </a>
    
    
      <a href="/2017/12/27/deep_learning_1/" class="article-nav-link">
        <strong class="article-nav-caption">Older</strong>
        <div class="article-nav-title">记一次竞赛题目有感——图像识别</div>
      </a>
    
  </nav>


            

                
                    
                        
                            

</article>
</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 春少 Blog</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="春少 Blog"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>
<script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>

  <script src="/fancybox/jquery.fancybox.min.js"></script>



  <script src="/js/tocbot.min.js"></script>
  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script src="/js/ocean.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"left","hOffset":0,"vOffset":-50},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"tagMode":false});</script></body>
</html>